"""
æ™ºèƒ½æœç´¢åŠ©æ‰‹ - LangGraph + Tavily + Human-in-the-Loopï¼ˆæ–°ç‰ˆ APIï¼‰
"""

import asyncio
from typing import TypedDict, Annotated

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver

from langgraph.types import interrupt, Command  # âœ… æ–° HITL API

import os
from dotenv import load_dotenv
from tavily import TavilyClient

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®šä¹‰çŠ¶æ€ç»“æ„
class SearchState(TypedDict):
    messages: Annotated[list, add_messages]  # åŒ…å«æ‰€æœ‰æ¶ˆæ¯çš„åˆ—è¡¨
    user_query: str        # ç”¨æˆ·æŸ¥è¯¢ï¼ˆçœŸå®ç”¨æˆ·è¾“å…¥ï¼‰
    search_query: str      # ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢ï¼ˆå¯è¢«äººä¿®æ”¹ç¡®è®¤ï¼‰
    search_results: str    # Tavilyæœç´¢ç»“æœ
    final_answer: str      # æœ€ç»ˆç­”æ¡ˆ
    step: str              # å½“å‰æ­¥éª¤

# åˆå§‹åŒ–æ¨¡å‹å’ŒTavilyå®¢æˆ·ç«¯
llm = ChatOpenAI(
    model=os.getenv("LLM_MODEL_ID", "gpt-4o-mini"),
    api_key=os.getenv("LLM_API_KEY"),
    base_url=os.getenv("LLM_BASE_URL", "https://api.openai.com/v1"),
    temperature=0.7
)
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def understand_query_node(state: SearchState) -> SearchState:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯ï¼ˆå†…ç½® HITL ç¡®è®¤ï¼‰"""

    # è·å–æœ€æ–°ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break

    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"

è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    resp = llm.invoke([SystemMessage(content=understand_prompt)])
    resp_text = resp.content

    # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½œä¸ºæœç´¢è¯
    search_query = user_message
    if "æœç´¢è¯ï¼š" in resp_text:
        search_query = resp_text.split("æœç´¢è¯ï¼š")[1].strip()
    elif "æœç´¢å…³é”®è¯ï¼š" in resp_text:
        search_query = resp_text.split("æœç´¢å…³é”®è¯ï¼š")[1].strip()

    # âœ… Human-in-the-Loopï¼šæš‚åœç­‰å¾…äººå·¥ç¡®è®¤/ä¿®æ”¹
    human_feedback = interrupt({
        "model_understanding": resp_text,
        "suggested_search_query": search_query,
        "prompt": "è¾“å…¥ yes ç»§ç»­ï¼›æˆ–ç›´æ¥è¾“å…¥æ–°çš„æœç´¢å…³é”®è¯ï¼š"
    })

    # æ¢å¤åï¼šå¦‚æœäººç±»è¾“å…¥ä¸æ˜¯ yes/ç©ºï¼Œåˆ™è¦†ç›–æœç´¢è¯
    if isinstance(human_feedback, str) and human_feedback.strip() and human_feedback.lower() != "yes":
        search_query = human_feedback.strip()

    return {
        "user_query": user_message,  # âœ… ä¿®æ­£ï¼šå­˜çœŸå®ç”¨æˆ·é—®é¢˜
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘ç†è§£æ‚¨çš„éœ€æ±‚ï¼š{resp_text}\nå°†ä½¿ç”¨æœç´¢è¯ï¼š{search_query}")]
    }

def tavily_search_node(state: SearchState) -> SearchState:
    """æ­¥éª¤2ï¼šä½¿ç”¨Tavily APIè¿›è¡ŒçœŸå®æœç´¢"""
    query = state["search_query"]
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {query}")
        response = tavily_client.search(
            query=query,
            search_depth="basic",
            include_answer=True,
            include_raw_content=False,
            max_results=5
        )

        # å¤„ç†æœç´¢ç»“æœ
        search_results = ""
        if response.get("answer"):
            search_results = f"ç»¼åˆç­”æ¡ˆï¼š\n{response['answer']}\n\n"

        if response.get("results"):
            search_results += "ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, result in enumerate(response["results"][:3], 1):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                search_results += f"{i}. {title}\n{content}\næ¥æºï¼š{url}\n\n"

        if not search_results:
            search_results = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content="âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼Œæ­£åœ¨ä¸ºæ‚¨æ•´ç†ç­”æ¡ˆ...")]
        }

    except Exception as e:
        error_msg = f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{error_msg}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜ï¼Œæˆ‘å°†åŸºäºå·²æœ‰çŸ¥è¯†ä¸ºæ‚¨å›ç­”")]
        }

def generate_answer_node(state: SearchState) -> SearchState:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    if state["step"] == "search_failed":
        fallback_prompt = f"""æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨çš„å›ç­”ï¼Œå¹¶è¯´æ˜è¿™æ˜¯åŸºäºå·²æœ‰çŸ¥è¯†çš„å›ç­”ã€‚"""
        response = llm.invoke([SystemMessage(content=fallback_prompt)])
        return {
            "final_answer": response.content,
            "step": "completed",
            "messages": [AIMessage(content=response.content)]
        }

    answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æœç´¢ç»“æœï¼š
{state['search_results']}

è¯·è¦æ±‚ï¼š
1. ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”
2. å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆæˆ–ä»£ç 
3. å¼•ç”¨é‡è¦ä¿¡æ¯çš„æ¥æº
4. å›ç­”è¦ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£
5. å¦‚æœæœç´¢ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œè¯·è¯´æ˜å¹¶æä¾›è¡¥å……å»ºè®®"""
    response = llm.invoke([SystemMessage(content=answer_prompt)])
    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }

# æ„å»ºæœç´¢å·¥ä½œæµ
def create_search_assistant():
    workflow = StateGraph(SearchState)
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)

    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)

    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app

async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼ˆæ”¯æŒ HITL æ¢å¤ï¼‰"""

    if not os.getenv("TAVILY_API_KEY"):
        print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®TAVILY_API_KEY")
        return

    app = create_search_assistant()
    print("ğŸ” æ™ºèƒ½æœç´¢åŠ©æ‰‹å¯åŠ¨ï¼ï¼ˆå·²å¯ç”¨ Human-in-the-Loopï¼‰")
    print("(è¾“å…¥ 'quit' é€€å‡º)\n")

    # å›ºå®š thread_idï¼Œä¿è¯å¤šè½®/ä¸­æ–­å¯æ¢å¤
    config = {"configurable": {"thread_id": "search-session-1"}}

    while True:
        user_input = input("ğŸ¤” æ‚¨æƒ³äº†è§£ä»€ä¹ˆ: ").strip()
        if user_input.lower() in ['quit', 'q', 'é€€å‡º', 'exit']:
            print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
            break
        if not user_input:
            continue

        print("\n" + "="*60)

        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": user_input,
            "search_query": "",
            "search_results": "",
            "final_answer": "",
            "step": "start"
        }

        # å¯èƒ½ä¼šå¤šæ¬¡â€œä¸­æ–­â†’æ¢å¤â€ï¼Œç›´åˆ°åˆ°è¾¾ END
        pending_resume = None
        finished = False

        while not finished:
            # é¦–æ¬¡æ­£å¸¸è·‘ï¼›ä¸­æ–­åç”¨ Command(resume=...)
            if pending_resume is None:
                stream_iter = app.stream(initial_state, config=config, stream_mode="updates")
            else:
                stream_iter = app.stream(Command(resume=pending_resume), config=config, stream_mode="updates")
                pending_resume = None

            interrupted = False

            for event in stream_iter:
                # 1) å¤„ç†ä¸­æ–­äº‹ä»¶ï¼ˆpayload åœ¨ __interrupt__ ä¸­ï¼‰
                intr = event.get("__interrupt__")
                if intr:
                    # __interrupt__ é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å« Interrupt çš„åºåˆ—ï¼›å–å‡ºå…¶ value
                    interrupt_obj = intr[0] if isinstance(intr, (list, tuple)) else intr
                    ivalue = getattr(interrupt_obj, "value", interrupt_obj)

                    print("\nğŸ›‘ Human-in-the-Loopï¼š")
                    if isinstance(ivalue, dict):
                        print(f"ğŸ§  æ¨¡å‹ç†è§£ä¸ºï¼š{ivalue.get('model_understanding')}")
                        print(f"ğŸ” å»ºè®®æœç´¢å…³é”®è¯ï¼š{ivalue.get('suggested_search_query')}")
                        print(ivalue.get("prompt", "ğŸ‘‰ è¾“å…¥ yes ç»§ç»­ï¼›æˆ–è¾“å…¥æ–°çš„æœç´¢å…³é”®è¯ï¼š"))
                    else:
                        print(ivalue)

                    pending_resume = input("> ").strip()
                    interrupted = True
                    break  # è·³å‡º forï¼Œä¸‹ä¸€è½®ç”¨ Command(resume=...) ç»§ç»­

                # 2) æ­£å¸¸èŠ‚ç‚¹å¢é‡äº‹ä»¶ï¼šæ‰“å° AI æ¶ˆæ¯
                for node_name, node_output in event.items():
                    if not isinstance(node_output, dict):
                        continue
                    msgs = node_output.get("messages") or []
                    if msgs:
                        latest_message = msgs[-1]
                        if isinstance(latest_message, AIMessage):
                            if node_name == "understand":
                                print(f"ğŸ§  ç†è§£é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "search":
                                print(f"ğŸ” æœç´¢é˜¶æ®µ: {latest_message.content}")
                            elif node_name == "answer":
                                print(f"\nğŸ’¡ æœ€ç»ˆå›ç­”:\n{latest_message.content}")

            if not interrupted:
                finished = True  # æœªè¢«ä¸­æ–­ï¼Œè¯´æ˜åˆ°è¾¾ END

        print("\n" + "="*60 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
